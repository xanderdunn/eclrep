{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "   \n",
    "# Import the mLSTM babbler model\n",
    "from unirep import babbler1900 as babbler\n",
    "    \n",
    "# Where model weights are stored.\n",
    "MODEL_WEIGHT_PATH = \"./data/1900_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:362: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:113: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:29: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/distributions/categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n",
      "Tensor(\"rnn_1/transpose_1:0\", shape=(12, ?, 1900), dtype=float32)\n",
      "(<tf.Tensor 'rnn_1/while/Exit_3:0' shape=(12, 1900) dtype=float32>, <tf.Tensor 'rnn_1/while/Exit_4:0' shape=(12, 1900) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "batch_size = 12\n",
    "model = babbler(batch_size=batch_size, model_path=MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that representations are reproducible\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load the saved representations\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "existing_seqs = pd.read_hdf(output_path, key=\"ids\").reset_index(drop=True)\n",
    "existing_reps = pd.read_hdf(output_path, key=\"reps\").reset_index(drop=True)\n",
    "assert existing_seqs.shape[0] == existing_reps.shape[0]\n",
    "assert np.array_equal(existing_seqs.index, existing_reps.index)\n",
    "\n",
    "# Create reprensetations for some seqs\n",
    "for index, row in tqdm(existing_seqs.iterrows(), total=existing_seqs.shape[0]):\n",
    "    check_rep_1 = model.get_rep(row[\"sequence\"])\n",
    "    check_rep_1 = np.concatenate((check_rep_1[0], check_rep_1[1], check_rep_1[2]))\n",
    "    check_rep_2 = model.get_rep(row[\"sequence\"])\n",
    "    check_rep_2 = np.concatenate((check_rep_2[0], check_rep_2[1], check_rep_2[2]))\n",
    "    true_rep = existing_reps.iloc[index].values\n",
    "\n",
    "    if not np.allclose(true_rep, check_rep_1, atol=0.0002) or not np.allclose(check_rep_1, check_rep_2, atol=0.0002):\n",
    "        true_check_diff = abs(np.sum(true_rep - check_rep_1))\n",
    "        self_run_diff = abs(np.sum(check_rep_1 - check_rep_2))\n",
    "        print(\"{}: {} difference with saved truth\".format(index, true_check_diff))\n",
    "        print(\"{}: {} difference with self comparison\".format(index, self_run_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save meta_graph\n",
    "# graph_def = tf.Session().graph_def\n",
    "# graph_def = tf.get_default_graph().as_graph_def() # Get the loaded babbler graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(100, 44), dtype=int32)\n",
      "<DatasetV1Adapter shapes: (44,), types: tf.int32>\n",
      "[ 0.01322009 -0.02335201  0.01794337 ...  0.04402846  0.03012068\n",
      "  0.04794597] [ 0.00847428 -0.00885868  0.0327703  ...  0.07334086  0.07954074\n",
      "  0.08330578] [ 1.7381712 -1.559962   6.0924277 ...  0.5680593  0.9212593  1.4379153]\n"
     ]
    }
   ],
   "source": [
    "from unirep import mLSTMCell1900, tf_get_shape\n",
    "\n",
    "class babbler1900():\n",
    "\n",
    "    def __init__(self,\n",
    "                 model_path=\"./data/1900_weights\",\n",
    "                 batch_size=1\n",
    "                 ):\n",
    "        self._rnn_size = 1900\n",
    "        self._vocab_size = 26\n",
    "        self._embed_dim = 10\n",
    "        self._wn = True\n",
    "        self._shuffle_buffer = 10000\n",
    "        self._model_path = model_path\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        # Get the input sequences\n",
    "        path = \"./data/stability_data\"\n",
    "        df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "        seqs = df[\"sequence\"].iloc[:100].values\n",
    "        seq_ints = [aa_seq_to_int(seq.strip())[:-1] for seq in seqs]\n",
    "        tf_tensor = tf.convert_to_tensor(seq_ints)\n",
    "        print(tf_tensor)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(tf_tensor)\n",
    "        dataset.batch(1)\n",
    "        print(dataset)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        input_tensor = iterator.get_next()\n",
    "        \n",
    "        # Batch size dimensional placeholder which gives the\n",
    "        # Lengths of the input sequence batch. Used to index into\n",
    "        # The final_hidden output and select the stop codon -1\n",
    "        # final hidden for the graph operation.\n",
    "        rnn = mLSTMCell1900(self._rnn_size,\n",
    "                    model_path=model_path,\n",
    "                        wn=self._wn)\n",
    "        zero_state = rnn.zero_state(self._batch_size, tf.float32)\n",
    "        single_zero = rnn.zero_state(1, tf.float32)\n",
    "\n",
    "        embed_matrix = tf.get_variable(\n",
    "            \"embed_matrix\", dtype=tf.float32, initializer=np.load(os.path.join(self._model_path, \"embed_matrix:0.npy\"))\n",
    "        )\n",
    "        embed_cell = tf.nn.embedding_lookup(embed_matrix, [input_tensor])\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self._zero_state = sess.run(zero_state)\n",
    "            self._single_zero = sess.run(single_zero)\n",
    "            \n",
    "        self._output, self._final_state = tf.nn.dynamic_rnn(\n",
    "            rnn,\n",
    "            embed_cell,\n",
    "            initial_state=self._zero_state,\n",
    "            swap_memory=True,\n",
    "            parallel_iterations=1\n",
    "        )\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "        #         final_cell, final_hidden, hs = sess.run([final_cell_ts, final_hidden_ts, hs_ts])\n",
    "            final_state_, hs = sess.run([self._final_state, self._output])\n",
    "\n",
    "            final_cell, final_hidden = final_state_\n",
    "            # Drop the batch dimension so it is just seq len by representation size\n",
    "            final_cell = final_cell[0]\n",
    "            final_hidden = final_hidden[0]\n",
    "            hs = hs[0]\n",
    "            avg_hidden = np.mean(hs, axis=0)\n",
    "            print(avg_hidden, final_hidden, final_cell)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = babbler1900()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "Tensor(\"Const_19:0\", shape=(100, 44), dtype=int32)\n",
      "<DatasetV1Adapter shapes: (44,), types: tf.int32>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.0132201 , -0.023352  ,  0.01794336, ...,  0.04402848,\n",
       "         0.03012069,  0.047946  ], dtype=float32),\n",
       " array([ 0.00847428, -0.00885865,  0.03277024, ...,  0.07334112,\n",
       "         0.07954072,  0.08330579], dtype=float32),\n",
       " array([ 1.7381716, -1.5599622,  6.092423 , ...,  0.5680599,  0.9212607,\n",
       "         1.4379159], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tf.data and tf.Variable to feed data into the inference step\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from unirep import aa_seq_to_int, initialize_uninitialized\n",
    "import pandas as pd\n",
    "\n",
    "#using a placeholder\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# data = np.random.sample((100,2))\n",
    "# tensor = tf.convert_to_tensor(data)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "# iter = dataset.make_one_shot_iterator()\n",
    "# el = iter.get_next()\n",
    "# with tf.Session() as sess:\n",
    "#     result = sess.run(el) # output [0.37454012 0.95071431]\n",
    "#     print(result)\n",
    "\n",
    "# tf.reset_default_graph() # Reset the graph\n",
    "\n",
    "def get_reps(model, seqs):\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         final_cell, final_hidden, hs = sess.run([final_cell_ts, final_hidden_ts, hs_ts])\n",
    "        final_state_, hs = sess.run([model._inference_final_state, model._inference_output],\n",
    "                                    feed_dict={\n",
    "                                        model._minibatch_x_placeholder: [seq_ints[0]]\n",
    "                                    }\n",
    "                                   )\n",
    "\n",
    "        final_cell, final_hidden = final_state_\n",
    "        # Drop the batch dimension so it is just seq len by representation size\n",
    "        final_cell = final_cell[0]\n",
    "        final_hidden = final_hidden[0]\n",
    "        hs = hs[0]\n",
    "        avg_hidden = np.mean(hs, axis=0)\n",
    "        return avg_hidden, final_hidden, final_cell\n",
    "    \n",
    "path = \"./data/stability_data\"\n",
    "df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "seqs = df[\"sequence\"].iloc[:100].values\n",
    "get_reps(model, seqs) # TODO: This is only the first 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "\n",
    "existing_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "if os.path.isfile(output_path):\n",
    "    print(\"Reading existing output file...\")\n",
    "    existing_output = pd.read_hdf(output_path, key=\"ids\")\n",
    "    print(\"Got {} existing data points\".format(existing_output.shape[0]))\n",
    "    duplicates = existing_output.duplicated(subset=[\"sequence\"])\n",
    "    assert True not in duplicates.values\n",
    "\n",
    "new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(\"Processing data from {}\".format(filename))\n",
    "        df = pd.read_table(os.path.join(path, filename))\n",
    "        model.get_rep(df[\"sequence\"].values)\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]): # TODO: Parallelize this\n",
    "            if index != 0 and index % 20 == 0:\n",
    "                assert new_ids_output.shape[0] == new_reps_output.shape[0]\n",
    "                if new_ids_output.shape[0] > 0:\n",
    "                    print(\"Appending {} points...\".format(new_ids_output.shape[0]))\n",
    "                    new_ids_output.to_hdf(output_path, index=False, mode=\"a\", key=\"ids\", format=\"table\", append=True)\n",
    "                    new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "                    new_reps_output.to_hdf(output_path, index=False, mode=\"a\", key=\"reps\", format=\"table\", append=True)\n",
    "                    new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "            # If there is no existing data or the existing data already contains this sequence, ignore it\n",
    "            if existing_output.empty or not row[\"sequence\"] in existing_output[\"sequence\"].values:\n",
    "                if model.is_valid_seq(row[\"sequence\"], max_len=500):\n",
    "                    unirep_fusion = model.get_rep(row[\"sequence\"])\n",
    "                    unirep_fusion = np.concatenate((unirep_fusion[0], unirep_fusion[1], unirep_fusion[2]))\n",
    "                    print(unirep_fusion.shape)\n",
    "                    if \"consensus_stability_score\" in df.columns:\n",
    "                        stability_score = row[\"consensus_stability_score\"]\n",
    "                    else:\n",
    "                        stability_score = row[\"stabilityscore\"]\n",
    "                    new_ids_output.loc[len(new_ids_output)]=[row[\"name\"], row[\"sequence\"], stability_score]\n",
    "                    new_reps_output.loc[len(new_reps_output)]=unirep_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_hdf(output_path, key=\"ids\")\n",
    "print(\"{} points in ids\".format(ids.shape[0]))\n",
    "reps = pd.read_hdf(output_path, key=\"reps\")\n",
    "print(\"{} points in reps\".format(reps.shape[0]))\n",
    "print(reps.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
