{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "    \n",
    "# Where model weights are stored.\n",
    "MODEL_WEIGHT_PATH = \"./data/1900_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 batches\n",
      "Getting EclRep representations for 6011 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 6011 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 829 sequences of length 46...\n",
      "Done\n",
      "Got (12851, 5700) results\n",
      "Checking that these results match the saved truth...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "603a92a5441643e883fd7095fb748864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11380), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unirep import mLSTMCell1900, tf_get_shape, aa_seq_to_int\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def is_valid_seq(seq, max_len=500):\n",
    "    \"\"\"\n",
    "    True if seq is valid for the babbler, False otherwise.\n",
    "    \"\"\"\n",
    "    l = len(seq)\n",
    "    valid_aas = \"MRHKDESTNQCUGPAVIFYWLO\"\n",
    "    if (l < max_len) and set(seq) <= set(valid_aas):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "class babbler1900():\n",
    "\n",
    "    def __init__(self, model_path=\"./data/1900_weights\", batch_size=500):\n",
    "        self._model_path = model_path\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._rnn = mLSTMCell1900(1900,\n",
    "                    model_path=self._model_path,\n",
    "                        wn=True)\n",
    "        zero_state = self._rnn.zero_state(self._batch_size, tf.float32)\n",
    "\n",
    "        self._embed_matrix = tf.get_variable(\n",
    "            \"embed_matrix\", dtype=tf.float32, initializer=np.load(os.path.join(self._model_path, \"embed_matrix:0.npy\"))\n",
    "        )\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self._zero_state = sess.run(zero_state)\n",
    "        \n",
    "    def get_reps(self, seqs):\n",
    "        seq_ints = [aa_seq_to_int(seq.strip())[:-1] for seq in seqs]\n",
    "        lengths = [len(x) for x in seq_ints]\n",
    "        tf_tensor = tf.convert_to_tensor(seq_ints)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(tf_tensor).batch(self._batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        input_tensor = iterator.get_next()\n",
    "\n",
    "        embed_cell = tf.nn.embedding_lookup(self._embed_matrix, input_tensor)\n",
    "        _output, _final_state = tf.nn.dynamic_rnn(\n",
    "            self._rnn,\n",
    "            embed_cell,\n",
    "            initial_state=self._zero_state,\n",
    "            swap_memory=True,\n",
    "            parallel_iterations=1\n",
    "        )\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            final_state_, hs = sess.run([_final_state, _output])\n",
    "            assert final_state_[0].shape[0] == self._batch_size\n",
    "\n",
    "            final_cell, final_hidden = final_state_\n",
    "            avg_hidden = np.array([np.mean(x, axis=0) for x in hs])\n",
    "            together = np.concatenate((avg_hidden, final_hidden, final_cell), axis=1)\n",
    "            return together\n",
    "\n",
    "# Given a pandas dataframe with a column \"sequence\", return a list of pandas dataframes grouped by sequence length\n",
    "def create_batches(seqs, batch_max_size=10000):\n",
    "    # Get the unique lengths of all these sequences\n",
    "    batches = []\n",
    "    lengths = seqs[\"sequence\"].apply(lambda x: len(x))\n",
    "    unique_lengths = lengths.unique()\n",
    "    for unique_length in unique_lengths:\n",
    "        boolean_mask = lengths == unique_length\n",
    "        seqs_of_length = seqs[boolean_mask]\n",
    "        if seqs_of_length.shape[0] > batch_max_size:\n",
    "            batches += np.array_split(seqs_of_length, 2)\n",
    "        else:\n",
    "            batches += [seqs_of_length]\n",
    "    print(\"There are {} batches\".format(len(batches)))\n",
    "    return batches\n",
    "    \n",
    "# Get representations for a numpy array of sequences where all sequences are the same length\n",
    "def inference_on_seqs_array(seqs):\n",
    "        tf.reset_default_graph()\n",
    "        model = babbler1900(batch_size=seqs.shape[0])\n",
    "        result = model.get_reps(seqs)\n",
    "        return result\n",
    "\n",
    "# Get representations for a pandas dataframe of sequences in coulmn \"sequence\"\n",
    "def inference_on_seqs(seqs):\n",
    "    # Check that all these sequences are valid\n",
    "    valid_func = lambda x: is_valid_seq(x)\n",
    "    valid_np = np.vectorize(valid_func)\n",
    "    valids = valid_np(seqs[\"sequence\"].values)\n",
    "    assert False not in valids\n",
    "    \n",
    "    batches = create_batches(seqs)\n",
    "    \n",
    "    ids = None\n",
    "    reps = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "    \n",
    "    for batch in batches:\n",
    "        print(\"Getting EclRep representations for {} sequences of length {}...\".format(batch.shape[0], len(batch.iloc[0][\"sequence\"])))\n",
    "        reps_new = inference_on_seqs_array(batch[\"sequence\"])\n",
    "        print(\"Done\")\n",
    "        reps = reps.append(pd.DataFrame(reps_new))\n",
    "        if ids is not None:\n",
    "            ids = ids.append(batch)\n",
    "        else:\n",
    "            ids = batch\n",
    "    return ids, reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/stability_data\"\n",
    "df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "ids, results = inference_on_seqs(df)\n",
    "print(\"Got {} results\".format(results.shape))\n",
    "assert results.shape[0] == seqs.shape[0]\n",
    "assert results.shape[1] == 5700\n",
    "\n",
    "# Check that representations are reproducible\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load the saved representations\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "existing_seqs = pd.read_hdf(output_path, key=\"ids\").reset_index(drop=True)\n",
    "existing_reps = pd.read_hdf(output_path, key=\"reps\").reset_index(drop=True)\n",
    "assert existing_seqs.shape[0] == existing_reps.shape[0]\n",
    "assert np.array_equal(existing_seqs.index, existing_reps.index)\n",
    "\n",
    "# Create reprensetations for some seqs\n",
    "print(\"Checking that these results match the saved truth...\")\n",
    "for index, row in tqdm(existing_seqs.iterrows(), total=existing_seqs.shape[0]):\n",
    "    check_rep = results.iloc[index].values\n",
    "    true_rep = existing_reps.iloc[index].values\n",
    "\n",
    "    if not np.allclose(true_rep, check_rep, atol=0.0001):\n",
    "        true_check_diff = abs(np.sum(true_rep - check_rep))\n",
    "        print(\"{}: {} difference with saved truth\".format(index, true_check_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12521 rows from rd3_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 6261 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 6260 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Processing 12196 rows from rd1_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 6098 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 6098 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Processing 19697 rows from rd4_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 9849 sequences of length 50...\n",
      "Done\n",
      "Getting EclRep representations for 9848 sequences of length 50...\n",
      "Done\n",
      "\n",
      "Processing 11769 rows from rd2_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 5885 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 5884 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Removed 86 duplicates\n",
      "Final rows: (56097, 5700)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "path = \"./data/stability_data\"\n",
    "output_ids_path = os.path.join(path, \"all_rds_ids.hdf\")\n",
    "output_reps_path = os.path.join(path, \"all_rds_reps.hdf\")\n",
    "\n",
    "ids = None\n",
    "reps = None\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\") and \"rd\" in filename:\n",
    "        df = pd.read_table(os.path.join(path, filename))\n",
    "        print(\"Processing {} rows from {}...\".format(df.shape[0], filename))\n",
    "        if \"consensus_stability_score\" in df.columns:\n",
    "            stability_name = \"consensus_stability_score\"\n",
    "        else:\n",
    "            stability_name = \"stabilityscore\"\n",
    "        df = df[[\"name\", \"sequence\", stability_name]]\n",
    "        df.rename(columns={'consensus_stability_score': 'stability', 'stabilityscore': 'stability'}, inplace=True)\n",
    "        ids_new, reps_new = inference_on_seqs(df)\n",
    "        if ids is None:\n",
    "            ids = ids_new\n",
    "            reps = reps_new\n",
    "        else:\n",
    "            ids = ids.append(ids_new)\n",
    "            reps = reps.append(reps_new)\n",
    "        print(\"\")\n",
    "           \n",
    "# Remove duplicates\n",
    "before_size = reps.shape[0]\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "reps.reset_index(drop=True, inplace=True)\n",
    "duplicated = ids.duplicated(subset=\"sequence\", keep=False)\n",
    "reps = reps[~duplicated]\n",
    "reps.reset_index(drop=True, inplace=True)\n",
    "ids = ids[~duplicated]\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "print(\"Removed {} duplicates\".format(before_size - reps.shape[0]))\n",
    "\n",
    "assert reps.shape[0] == ids.shape[0]\n",
    "print(\"Final rows: {}\".format(reps.shape))\n",
    "print(\"Saving to file...\")\n",
    "ids.to_hdf(output_ids_path, index=False, mode=\"w\", key=\"ids\", format=\"fixed\")\n",
    "reps.to_hdf(output_reps_path, index=False, mode=\"w\", key=\"reps\", format=\"fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'No object named ids in the file'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-9beefe693f4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} points in ids\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_hdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"reps\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{} points in reps\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mread_hdf\u001b[0;34m(path_or_buf, key, mode, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m                                      'contains multiple datasets.')\n\u001b[1;32m    393\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcandidate_only_group\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v_pathname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_close\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauto_close\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m     \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;31m# if there is an error, close the store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/pandas/io/pytables.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, key, where, start, stop, columns, iterator, chunksize, auto_close, **kwargs)\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0mgroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No object named %s in the file'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0;31m# create the storer and axes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'No object named ids in the file'"
     ]
    }
   ],
   "source": [
    "ids = pd.read_hdf(output_ids_path)\n",
    "print(\"{} in ids\".format(ids.shape))\n",
    "reps = pd.read_hdf(output_reps_path)\n",
    "print(\"{} in reps\".format(reps.shape))\n",
    "print(reps.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
