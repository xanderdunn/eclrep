{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "   \n",
    "# Import the mLSTM babbler model\n",
    "from unirep import babbler1900 as babbler\n",
    "    \n",
    "# Where model weights are stored.\n",
    "MODEL_WEIGHT_PATH = \"./data/1900_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:362: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:113: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:29: Categorical.__init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/distributions/categorical.py:242: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.random.categorical instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 12\n",
    "model = babbler(batch_size=batch_size, model_path=MODEL_WEIGHT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f874c3d777894fb48152521ee931c14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11380), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check that representations are reproducible\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load the saved representations\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "existing_seqs = pd.read_hdf(output_path, key=\"ids\").reset_index(drop=True)\n",
    "existing_reps = pd.read_hdf(output_path, key=\"reps\").reset_index(drop=True)\n",
    "assert existing_seqs.shape[0] == existing_reps.shape[0]\n",
    "assert np.array_equal(existing_seqs.index, existing_reps.index)\n",
    "\n",
    "# Create reprensetations for some seqs\n",
    "for index, row in tqdm(existing_seqs.iterrows(), total=existing_seqs.shape[0]):\n",
    "    check_rep_1 = model.get_rep(row[\"sequence\"])\n",
    "    check_rep_1 = np.concatenate((check_rep_1[0], check_rep_1[1], check_rep_1[2]))\n",
    "    check_rep_2 = model.get_rep(row[\"sequence\"])\n",
    "    check_rep_2 = np.concatenate((check_rep_2[0], check_rep_2[1], check_rep_2[2]))\n",
    "    true_rep = existing_reps.iloc[index].values\n",
    "\n",
    "    if not np.allclose(true_rep, check_rep_1, atol=0.0002) or not np.allclose(check_rep_1, check_rep_2, atol=0.0002):\n",
    "        true_check_diff = abs(np.sum(true_rep - check_rep_1))\n",
    "        self_run_diff = abs(np.sum(check_rep_1 - check_rep_2))\n",
    "        print(\"{}: {} difference with saved truth\".format(index, true_check_diff))\n",
    "        print(\"{}: {} difference with self comparison\".format(index, self_run_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading existing output file...\n",
      "Got 11380 existing data points\n",
      "Processing data from ssm2_stability_scores.txt\n",
      "(12851,)\n",
      "1.3.0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-40f5af5a08ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing data from {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sequence\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# TODO: Parallelize this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/unirep.py\u001b[0m in \u001b[0;36mget_rep\u001b[0;34m(self, seqs)\u001b[0m\n\u001b[1;32m    403\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVERSION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mtf_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m         \u001b[0mtf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0mtf_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0maa_seq_to_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "\n",
    "existing_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "if os.path.isfile(output_path):\n",
    "    print(\"Reading existing output file...\")\n",
    "    existing_output = pd.read_hdf(output_path, key=\"ids\")\n",
    "    print(\"Got {} existing data points\".format(existing_output.shape[0]))\n",
    "    duplicates = existing_output.duplicated(subset=[\"sequence\"])\n",
    "    assert True not in duplicates.values\n",
    "\n",
    "new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(\"Processing data from {}\".format(filename))\n",
    "        df = pd.read_table(os.path.join(path, filename))\n",
    "        model.get_rep(df[\"sequence\"].values)\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]): # TODO: Parallelize this\n",
    "            if index != 0 and index % 20 == 0:\n",
    "                assert new_ids_output.shape[0] == new_reps_output.shape[0]\n",
    "                if new_ids_output.shape[0] > 0:\n",
    "                    print(\"Appending {} points...\".format(new_ids_output.shape[0]))\n",
    "                    new_ids_output.to_hdf(output_path, index=False, mode=\"a\", key=\"ids\", format=\"table\", append=True)\n",
    "                    new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "                    new_reps_output.to_hdf(output_path, index=False, mode=\"a\", key=\"reps\", format=\"table\", append=True)\n",
    "                    new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "            # If there is no existing data or the existing data already contains this sequence, ignore it\n",
    "            if existing_output.empty or not row[\"sequence\"] in existing_output[\"sequence\"].values:\n",
    "                if model.is_valid_seq(row[\"sequence\"], max_len=500):\n",
    "                    unirep_fusion = model.get_rep(row[\"sequence\"])\n",
    "                    unirep_fusion = np.concatenate((unirep_fusion[0], unirep_fusion[1], unirep_fusion[2]))\n",
    "                    print(unirep_fusion.shape)\n",
    "                    if \"consensus_stability_score\" in df.columns:\n",
    "                        stability_score = row[\"consensus_stability_score\"]\n",
    "                    else:\n",
    "                        stability_score = row[\"stabilityscore\"]\n",
    "                    new_ids_output.loc[len(new_ids_output)]=[row[\"name\"], row[\"sequence\"], stability_score]\n",
    "                    new_reps_output.loc[len(new_reps_output)]=unirep_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ids = pd.read_hdf(output_path, key=\"ids\")\n",
    "print(\"{} points in ids\".format(ids.shape[0]))\n",
    "reps = pd.read_hdf(output_path, key=\"reps\")\n",
    "print(\"{} points in reps\".format(reps.shape[0]))\n",
    "print(reps.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
