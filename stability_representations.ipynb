{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "    \n",
    "# Where model weights are stored.\n",
    "MODEL_WEIGHT_PATH = \"./data/1900_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions and classes for feed-forward network\n",
    "import os\n",
    "from unirep import mLSTMCell1900, tf_get_shape\n",
    "from data_utils import aa_seq_to_int\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def is_valid_seq(seq, max_len=500):\n",
    "    \"\"\"\n",
    "    True if seq is valid for the babbler, False otherwise.\n",
    "    \"\"\"\n",
    "    l = len(seq)\n",
    "    valid_aas = \"MRHKDESTNQCUGPAVIFYWLO\"\n",
    "    if (l < max_len) and set(seq) <= set(valid_aas):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "class babbler1900():\n",
    "\n",
    "    def __init__(self, model_path=\"./data/1900_weights\", batch_size=500):\n",
    "        self._model_path = model_path\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._rnn = mLSTMCell1900(1900,\n",
    "                    model_path=self._model_path,\n",
    "                        wn=True)\n",
    "        zero_state = self._rnn.zero_state(self._batch_size, tf.float32)\n",
    "\n",
    "        self._embed_matrix = tf.get_variable(\n",
    "            \"embed_matrix\", dtype=tf.float32, initializer=np.load(os.path.join(self._model_path, \"embed_matrix:0.npy\"))\n",
    "        )\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self._zero_state = sess.run(zero_state)\n",
    "        \n",
    "    def get_reps(self, seqs):\n",
    "        seq_ints = [aa_seq_to_int(seq.strip())[:-1] for seq in seqs]\n",
    "        lengths = [len(x) for x in seq_ints]\n",
    "        tf_tensor = tf.convert_to_tensor(seq_ints)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(tf_tensor).batch(self._batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        input_tensor = iterator.get_next()\n",
    "\n",
    "        embed_cell = tf.nn.embedding_lookup(self._embed_matrix, input_tensor)\n",
    "        _output, _final_state = tf.nn.dynamic_rnn(\n",
    "            self._rnn,\n",
    "            embed_cell,\n",
    "            initial_state=self._zero_state,\n",
    "            swap_memory=True,\n",
    "            parallel_iterations=1\n",
    "        )\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            final_state_, hs = sess.run([_final_state, _output])\n",
    "            assert final_state_[0].shape[0] == self._batch_size\n",
    "\n",
    "            final_cell, final_hidden = final_state_\n",
    "            avg_hidden = np.array([np.mean(x, axis=0) for x in hs])\n",
    "            together = np.concatenate((avg_hidden, final_hidden, final_cell), axis=1)\n",
    "            return together\n",
    "\n",
    "# Given a pandas dataframe with a column \"sequence\", return a list of pandas dataframes grouped by sequence length\n",
    "def create_batches(seqs, batch_max_size=10000):\n",
    "    # Get the unique lengths of all these sequences\n",
    "    batches = []\n",
    "    lengths = seqs[\"sequence\"].apply(lambda x: len(x))\n",
    "    unique_lengths = lengths.unique()\n",
    "    for unique_length in unique_lengths:\n",
    "        boolean_mask = lengths == unique_length\n",
    "        seqs_of_length = seqs[boolean_mask]\n",
    "        if seqs_of_length.shape[0] > batch_max_size:\n",
    "            batches += np.array_split(seqs_of_length, 2)\n",
    "        else:\n",
    "            batches += [seqs_of_length]\n",
    "    print(\"There are {} batches\".format(len(batches)))\n",
    "    return batches\n",
    "    \n",
    "# Get representations for a numpy array of sequences where all sequences are the same length\n",
    "def inference_on_seqs_array(seqs):\n",
    "        tf.reset_default_graph()\n",
    "        model = babbler1900(batch_size=seqs.shape[0])\n",
    "        result = model.get_reps(seqs)\n",
    "        return result\n",
    "\n",
    "# Get representations for a pandas dataframe of sequences in coulmn \"sequence\"\n",
    "def inference_on_seqs(seqs):\n",
    "    # Check that all these sequences are valid\n",
    "    valid_func = lambda x: is_valid_seq(x)\n",
    "    valid_np = np.vectorize(valid_func)\n",
    "    valids = valid_np(seqs[\"sequence\"].values)\n",
    "    assert False not in valids\n",
    "    \n",
    "    batches = create_batches(seqs)\n",
    "    \n",
    "    ids = None\n",
    "    reps = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "    \n",
    "    for batch in batches:\n",
    "        print(\"Getting EclRep representations for {} sequences of length {}...\".format(batch.shape[0], len(batch.iloc[0][\"sequence\"])))\n",
    "        reps_new = inference_on_seqs_array(batch[\"sequence\"])\n",
    "        print(\"Done\")\n",
    "        reps = reps.append(pd.DataFrame(reps_new))\n",
    "        if ids is not None:\n",
    "            ids = ids.append(batch)\n",
    "        else:\n",
    "            ids = batch\n",
    "    return ids, reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 batches\n",
      "Getting EclRep representations for 6011 sequences of length 43...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-2-8b95a07fc8f5>:52: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /tf/notebooks/unirep.py:113: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "Done\n",
      "Getting EclRep representations for 6011 sequences of length 43...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Check that representations are reproducible\n",
    "# This checks ~11,000 representations \n",
    "path = \"./data/stability_data\"\n",
    "df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "ids, results = inference_on_seqs(df)\n",
    "print(\"Got {} results\".format(results.shape))\n",
    "assert results.shape[0] == seqs.shape[0]\n",
    "assert results.shape[1] == 5700\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load the saved representations\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "existing_seqs = pd.read_hdf(output_path, key=\"ids\").reset_index(drop=True)\n",
    "existing_reps = pd.read_hdf(output_path, key=\"reps\").reset_index(drop=True)\n",
    "assert existing_seqs.shape[0] == existing_reps.shape[0]\n",
    "assert np.array_equal(existing_seqs.index, existing_reps.index)\n",
    "\n",
    "# Create reprensetations for some seqs\n",
    "print(\"Checking that these results match the saved truth...\")\n",
    "for index, row in tqdm(existing_seqs.iterrows(), total=existing_seqs.shape[0]):\n",
    "    check_rep = results.iloc[index].values\n",
    "    true_rep = existing_reps.iloc[index].values\n",
    "\n",
    "    if not np.allclose(true_rep, check_rep, atol=0.0001):\n",
    "        true_check_diff = abs(np.sum(true_rep - check_rep))\n",
    "        print(\"{}: {} difference with saved truth\".format(index, true_check_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 12521 rows from rd3_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 6261 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 6260 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Processing 12196 rows from rd1_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 6098 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 6098 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Processing 19697 rows from rd4_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 9849 sequences of length 50...\n",
      "Done\n",
      "Getting EclRep representations for 9848 sequences of length 50...\n",
      "Done\n",
      "\n",
      "Processing 11769 rows from rd2_stability_scores.txt...\n",
      "There are 2 batches\n",
      "Getting EclRep representations for 5885 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 5884 sequences of length 43...\n",
      "Done\n",
      "\n",
      "Removed 86 duplicates\n",
      "Final rows: (56097, 5700)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "path = \"./data/stability_data\"\n",
    "output_ids_path = os.path.join(path, \"all_rds_ids.hdf\")\n",
    "output_reps_path = os.path.join(path, \"all_rds_reps.hdf\")\n",
    "\n",
    "ids = None\n",
    "reps = None\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\") and \"rd\" in filename:\n",
    "        df = pd.read_table(os.path.join(path, filename))\n",
    "        print(\"Processing {} rows from {}...\".format(df.shape[0], filename))\n",
    "        if \"consensus_stability_score\" in df.columns:\n",
    "            stability_name = \"consensus_stability_score\"\n",
    "        else:\n",
    "            stability_name = \"stabilityscore\"\n",
    "        df = df[[\"name\", \"sequence\", stability_name]]\n",
    "        df.rename(columns={'consensus_stability_score': 'stability', 'stabilityscore': 'stability'}, inplace=True)\n",
    "        ids_new, reps_new = inference_on_seqs(df)\n",
    "        if ids is None:\n",
    "            ids = ids_new\n",
    "            reps = reps_new\n",
    "        else:\n",
    "            ids = ids.append(ids_new)\n",
    "            reps = reps.append(reps_new)\n",
    "        print(\"\")\n",
    "           \n",
    "# Remove duplicates\n",
    "before_size = reps.shape[0]\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "reps.reset_index(drop=True, inplace=True)\n",
    "duplicated = ids.duplicated(subset=\"sequence\", keep=False)\n",
    "reps = reps[~duplicated]\n",
    "reps.reset_index(drop=True, inplace=True)\n",
    "ids = ids[~duplicated]\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "print(\"Removed {} duplicates\".format(before_size - reps.shape[0]))\n",
    "\n",
    "assert reps.shape[0] == ids.shape[0]\n",
    "print(\"Final rows: {}\".format(reps.shape))\n",
    "print(\"Saving to file...\")\n",
    "ids.to_hdf(output_ids_path, index=False, mode=\"w\", key=\"ids\", format=\"fixed\")\n",
    "reps.to_hdf(output_reps_path, index=False, mode=\"w\", key=\"reps\", format=\"fixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56097, 3) in ids\n",
      "(56097, 5700) in reps\n",
      "0       0.016056\n",
      "1      -0.006879\n",
      "2       0.039395\n",
      "3       0.003077\n",
      "4      -0.195405\n",
      "5       0.052034\n",
      "6      -0.292709\n",
      "7      -0.009351\n",
      "8      -0.019956\n",
      "9       0.099939\n",
      "10      0.226287\n",
      "11      0.007669\n",
      "12      0.075929\n",
      "13      0.018608\n",
      "14      0.017757\n",
      "15      0.006822\n",
      "16      0.021515\n",
      "17      0.027132\n",
      "18      0.161061\n",
      "19     -0.054803\n",
      "20     -0.008328\n",
      "21      0.040010\n",
      "22      0.069497\n",
      "23     -0.052672\n",
      "24      0.064652\n",
      "25     -0.013834\n",
      "26     -0.023834\n",
      "27      0.021660\n",
      "28      0.005517\n",
      "29     -0.058577\n",
      "          ...   \n",
      "5670   -0.547642\n",
      "5671   -0.790233\n",
      "5672   -0.147351\n",
      "5673    0.277292\n",
      "5674   -0.392673\n",
      "5675   -1.994595\n",
      "5676   -1.793046\n",
      "5677    0.763829\n",
      "5678    0.178580\n",
      "5679   -0.860683\n",
      "5680    0.532057\n",
      "5681   -2.700825\n",
      "5682   -1.327123\n",
      "5683   -0.601100\n",
      "5684   -0.269338\n",
      "5685    2.285370\n",
      "5686   -0.213337\n",
      "5687   -0.526173\n",
      "5688   -2.413786\n",
      "5689   -0.301574\n",
      "5690    0.171033\n",
      "5691    0.508388\n",
      "5692   -0.960038\n",
      "5693    1.848754\n",
      "5694   -3.980704\n",
      "5695    1.108199\n",
      "5696   -0.139096\n",
      "5697    0.587404\n",
      "5698   -1.571620\n",
      "5699   -0.087265\n",
      "Name: 0, Length: 5700, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "ids = pd.read_hdf(output_ids_path)\n",
    "print(\"{} in ids\".format(ids.shape))\n",
    "reps = pd.read_hdf(output_reps_path)\n",
    "print(\"{} in reps\".format(reps.shape))\n",
    "print(reps.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
