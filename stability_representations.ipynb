{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set seeds\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "    \n",
    "# Where model weights are stored.\n",
    "MODEL_WEIGHT_PATH = \"./data/1900_weights\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting EclRep representations for 12022 sequences of length 43...\n",
      "Done\n",
      "Getting EclRep representations for 829 sequences of length 46...\n",
      "Done\n",
      "Got (12851, 5700) results\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a23fa7b26a48678c69544d584efe8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11380), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from unirep import mLSTMCell1900, tf_get_shape, aa_seq_to_int\n",
    "import pandas as pd\n",
    "\n",
    "class babbler1900():\n",
    "\n",
    "    def __init__(self, model_path=\"./data/1900_weights\", batch_size=500):\n",
    "        self._model_path = model_path\n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self._rnn = mLSTMCell1900(1900,\n",
    "                    model_path=self._model_path,\n",
    "                        wn=True)\n",
    "        zero_state = self._rnn.zero_state(self._batch_size, tf.float32)\n",
    "\n",
    "        self._embed_matrix = tf.get_variable(\n",
    "            \"embed_matrix\", dtype=tf.float32, initializer=np.load(os.path.join(self._model_path, \"embed_matrix:0.npy\"))\n",
    "        )\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            self._zero_state = sess.run(zero_state)\n",
    "        \n",
    "    def get_reps(self, seqs):\n",
    "        # Get the input sequences\n",
    "        seq_ints = [aa_seq_to_int(seq.strip())[:-1] for seq in seqs]\n",
    "        lengths = [len(x) for x in seq_ints]\n",
    "        tf_tensor = tf.convert_to_tensor(seq_ints)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(tf_tensor).batch(self._batch_size)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        input_tensor = iterator.get_next()\n",
    "\n",
    "        embed_cell = tf.nn.embedding_lookup(self._embed_matrix, input_tensor)\n",
    "        _output, _final_state = tf.nn.dynamic_rnn(\n",
    "            self._rnn,\n",
    "            embed_cell,\n",
    "            initial_state=self._zero_state,\n",
    "            swap_memory=True,\n",
    "            parallel_iterations=1\n",
    "        )\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            final_state_, hs = sess.run([_final_state, _output])\n",
    "            assert final_state_[0].shape[0] == self._batch_size\n",
    "\n",
    "            final_cell, final_hidden = final_state_\n",
    "            avg_hidden = np.array([np.mean(x, axis=0) for x in hs])\n",
    "            together = np.concatenate((avg_hidden, final_hidden, final_cell), axis=1)\n",
    "            return together\n",
    "\n",
    "# Create nets and run inference on a given list of sequences\n",
    "def inference_on_seqs(seqs):\n",
    "    len_func = lambda x: len(x)\n",
    "    len_np = np.vectorize(len_func)\n",
    "    seq_lengths = len_np(seqs)\n",
    "    unique_lengths = np.unique(seq_lengths)\n",
    "    results = None\n",
    "    for unique_length in unique_lengths:\n",
    "        boolean_mask = seq_lengths == unique_length\n",
    "        seqs_of_length = seqs[boolean_mask]\n",
    "        print(\"Getting EclRep representations for {} sequences of length {}...\".format(seqs_of_length.shape[0], unique_length))\n",
    "        tf.reset_default_graph()\n",
    "        model = babbler1900(batch_size=seqs_of_length.shape[0])\n",
    "        result = model.get_reps(seqs_of_length)\n",
    "        print(\"Done\")\n",
    "        if results is not None:\n",
    "            results = np.concatenate((results, result))\n",
    "        else:\n",
    "            results = result\n",
    "    return results\n",
    "    \n",
    "    \n",
    "path = \"./data/stability_data\"\n",
    "df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "seqs = df[\"sequence\"].values\n",
    "results = inference_on_seqs(seqs)\n",
    "print(\"Got {} results\".format(results.shape))\n",
    "assert results.shape[0] == seqs.shape[0]\n",
    "assert results.shape[1] == 5700\n",
    "\n",
    "# Check that representations are reproducible\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Load the saved representations\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "existing_seqs = pd.read_hdf(output_path, key=\"ids\").reset_index(drop=True)\n",
    "existing_reps = pd.read_hdf(output_path, key=\"reps\").reset_index(drop=True)\n",
    "assert existing_seqs.shape[0] == existing_reps.shape[0]\n",
    "assert np.array_equal(existing_seqs.index, existing_reps.index)\n",
    "\n",
    "# Create reprensetations for some seqs\n",
    "print(\"Checking that these results match the saved truth...\")\n",
    "for index, row in tqdm(existing_seqs.iterrows(), total=existing_seqs.shape[0]):\n",
    "    check_rep = results[index]\n",
    "    true_rep = existing_reps.iloc[index].values\n",
    "\n",
    "    if not np.allclose(true_rep, check_rep, atol=0.001):\n",
    "        true_check_diff = abs(np.sum(true_rep - check_rep))\n",
    "        print(\"{}: {} difference with saved truth\".format(index, true_check_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "Tensor(\"Const_19:0\", shape=(100, 44), dtype=int32)\n",
      "<DatasetV1Adapter shapes: (44,), types: tf.int32>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 0.0132201 , -0.023352  ,  0.01794336, ...,  0.04402848,\n",
       "         0.03012069,  0.047946  ], dtype=float32),\n",
       " array([ 0.00847428, -0.00885865,  0.03277024, ...,  0.07334112,\n",
       "         0.07954072,  0.08330579], dtype=float32),\n",
       " array([ 1.7381716, -1.5599622,  6.092423 , ...,  0.5680599,  0.9212607,\n",
       "         1.4379159], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use tf.data and tf.Variable to feed data into the inference step\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from unirep import aa_seq_to_int, initialize_uninitialized\n",
    "import pandas as pd\n",
    "\n",
    "#using a placeholder\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# data = np.random.sample((100,2))\n",
    "# tensor = tf.convert_to_tensor(data)\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(tensor)\n",
    "# iter = dataset.make_one_shot_iterator()\n",
    "# el = iter.get_next()\n",
    "# with tf.Session() as sess:\n",
    "#     result = sess.run(el) # output [0.37454012 0.95071431]\n",
    "#     print(result)\n",
    "\n",
    "# tf.reset_default_graph() # Reset the graph\n",
    "\n",
    "def get_reps(model, seqs):\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "#         final_cell, final_hidden, hs = sess.run([final_cell_ts, final_hidden_ts, hs_ts])\n",
    "        final_state_, hs = sess.run([model._inference_final_state, model._inference_output],\n",
    "                                    feed_dict={\n",
    "                                        model._minibatch_x_placeholder: [seq_ints[0]]\n",
    "                                    }\n",
    "                                   )\n",
    "\n",
    "        final_cell, final_hidden = final_state_\n",
    "        # Drop the batch dimension so it is just seq len by representation size\n",
    "        final_cell = final_cell[0]\n",
    "        final_hidden = final_hidden[0]\n",
    "        hs = hs[0]\n",
    "        avg_hidden = np.mean(hs, axis=0)\n",
    "        return avg_hidden, final_hidden, final_cell\n",
    "    \n",
    "path = \"./data/stability_data\"\n",
    "df = pd.read_table(os.path.join(path, \"ssm2_stability_scores.txt\"))\n",
    "seqs = df[\"sequence\"].iloc[:100].values\n",
    "get_reps(model, seqs) # TODO: This is only the first 100 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "path = \"./data/stability_data\"\n",
    "output_path = os.path.join(path, \"stability_with_unirep_fusion.hdf\")\n",
    "\n",
    "existing_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "if os.path.isfile(output_path):\n",
    "    print(\"Reading existing output file...\")\n",
    "    existing_output = pd.read_hdf(output_path, key=\"ids\")\n",
    "    print(\"Got {} existing data points\".format(existing_output.shape[0]))\n",
    "    duplicates = existing_output.duplicated(subset=[\"sequence\"])\n",
    "    assert True not in duplicates.values\n",
    "\n",
    "new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(\"Processing data from {}\".format(filename))\n",
    "        df = pd.read_table(os.path.join(path, filename))\n",
    "        model.get_rep(df[\"sequence\"].values)\n",
    "        for index, row in tqdm(df.iterrows(), total=df.shape[0]): # TODO: Parallelize this\n",
    "            if index != 0 and index % 20 == 0:\n",
    "                assert new_ids_output.shape[0] == new_reps_output.shape[0]\n",
    "                if new_ids_output.shape[0] > 0:\n",
    "                    print(\"Appending {} points...\".format(new_ids_output.shape[0]))\n",
    "                    new_ids_output.to_hdf(output_path, index=False, mode=\"a\", key=\"ids\", format=\"table\", append=True)\n",
    "                    new_ids_output = pd.DataFrame(columns=[\"name\", \"sequence\", \"stability\"])\n",
    "                    new_reps_output.to_hdf(output_path, index=False, mode=\"a\", key=\"reps\", format=\"table\", append=True)\n",
    "                    new_reps_output = pd.DataFrame(columns=list(range(0, 5700)))\n",
    "            # If there is no existing data or the existing data already contains this sequence, ignore it\n",
    "            if existing_output.empty or not row[\"sequence\"] in existing_output[\"sequence\"].values:\n",
    "                if model.is_valid_seq(row[\"sequence\"], max_len=500):\n",
    "                    unirep_fusion = model.get_rep(row[\"sequence\"])\n",
    "                    unirep_fusion = np.concatenate((unirep_fusion[0], unirep_fusion[1], unirep_fusion[2]))\n",
    "                    print(unirep_fusion.shape)\n",
    "                    if \"consensus_stability_score\" in df.columns:\n",
    "                        stability_score = row[\"consensus_stability_score\"]\n",
    "                    else:\n",
    "                        stability_score = row[\"stabilityscore\"]\n",
    "                    new_ids_output.loc[len(new_ids_output)]=[row[\"name\"], row[\"sequence\"], stability_score]\n",
    "                    new_reps_output.loc[len(new_reps_output)]=unirep_fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = pd.read_hdf(output_path, key=\"ids\")\n",
    "print(\"{} points in ids\".format(ids.shape[0]))\n",
    "reps = pd.read_hdf(output_path, key=\"reps\")\n",
    "print(\"{} points in reps\".format(reps.shape[0]))\n",
    "print(reps.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
